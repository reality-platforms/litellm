model_list:
  # OpenAI
  - model_name: gpt-4
    litellm_params:
      model: gpt-4
      custom_llm_provider: openai
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gpt-4-turbo-preview
    litellm_params:
      model: gpt-4-turbo-preview
      custom_llm_provider: openai
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gpt-4o
    litellm_params:
      model: gpt-4o
      custom_llm_provider: openai
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gpt-4o-mini
    litellm_params:
      model: gpt-4o-mini
      custom_llm_provider: openai
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      custom_llm_provider: openai
      api_key: os.environ/OPENAI_API_KEY
      
  # Fireworks AI
  - model_name: llama-v3p1-8b-instruct
    litellm_params:
      model: fireworks_ai/accounts/fireworks/models/llama-v3p1-8b-instruct
      api_base: https://api.fireworks.ai/inference/v1
      api_key: os.environ/FIREWORKS_API_KEY
  - model_name: llama-v3p1-70b-instruct
    litellm_params:
      model: fireworks_ai/accounts/fireworks/models/llama-v3p1-70b-instruct
      api_base: https://api.fireworks.ai/inference/v1
      api_key: os.environ/FIREWORKS_API_KEY
  - model_name: llama-v3p3-70b-instruct
    litellm_params:
      model: fireworks_ai/accounts/fireworks/models/llama-v3p3-70b-instruct
      api_base: https://api.fireworks.ai/inference/v1
      api_key: os.environ/FIREWORKS_API_KEY
  - model_name: llama-v3p2-90b-vision-instruct
    litellm_params:
      model: fireworks_ai/accounts/fireworks/models/llama-v3p2-90b-vision-instruct
      api_base: https://api.fireworks.ai/inference/v1
      api_key: os.environ/FIREWORKS_API_KEY
  - model_name: llama-v3p1-405b-instruct
    litellm_params:
      model: fireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct
      api_base: https://api.fireworks.ai/inference/v1
      api_key: os.environ/FIREWORKS_API_KEY
  - model_name: mixtral-8x22b-instruct
    litellm_params:
      model: fireworks_ai/accounts/fireworks/models/mixtral-8x22b-instruct
      api_base: https://api.fireworks.ai/inference/v1
      api_key: os.environ/FIREWORKS_API_KEY
# litellm_settings:
#   # set_verbose: True  # Uncomment this if you want to see verbose logs; not recommended in production
#   drop_params: True
#   # max_budget: 100 
#   # budget_duration: 30d
#   num_retries: 5
#   request_timeout: 600
#   telemetry: False
#   context_window_fallbacks: [{"gpt-3.5-turbo": ["gpt-3.5-turbo-large"]}]
#   default_team_settings: 
#     - team_id: team-1
#       success_callback: ["langfuse"]
#       failure_callback: ["langfuse"]
#       langfuse_public_key: os.environ/LANGFUSE_PROJECT1_PUBLIC # Project 1
#       langfuse_secret: os.environ/LANGFUSE_PROJECT1_SECRET # Project 1
#     - team_id: team-2
#       success_callback: ["langfuse"]
#       failure_callback: ["langfuse"]
#       langfuse_public_key: os.environ/LANGFUSE_PROJECT2_PUBLIC # Project 2
#       langfuse_secret: os.environ/LANGFUSE_PROJECT2_SECRET # Project 2
#       langfuse_host: https://us.cloud.langfuse.com

# For /fine_tuning/jobs endpoints
# finetune_settings:
#   - custom_llm_provider: azure
#     api_base: https://exampleopenaiendpoint-production.up.railway.app
#     api_key: fake-key
#     api_version: "2023-03-15-preview"
#   - custom_llm_provider: openai
#     api_key: os.environ/OPENAI_API_KEY

# for /files endpoints
# files_settings:
#   - custom_llm_provider: azure
#     api_base: https://exampleopenaiendpoint-production.up.railway.app
#     api_key: fake-key
#     api_version: "2023-03-15-preview"
#   - custom_llm_provider: openai
#     api_key: os.environ/OPENAI_API_KEY

# router_settings:
#   routing_strategy: usage-based-routing-v2 
#   redis_host: os.environ/REDIS_HOST
#   redis_password: os.environ/REDIS_PASSWORD
#   redis_port: os.environ/REDIS_PORT
#   enable_pre_call_checks: true
#   model_group_alias: {"my-special-fake-model-alias-name": "fake-openai-endpoint-3"} 

# general_settings: 
  # master_key: sk-1234 # [OPTIONAL] Use to enforce auth on proxy. See - https://docs.litellm.ai/docs/proxy/virtual_keys
  # # store_model_in_db: True
  # proxy_budget_rescheduler_min_time: 60
  # proxy_budget_rescheduler_max_time: 64
  # proxy_batch_write_at: 1
  # database_connection_pool_limit: 10
  # database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # [OPTIONAL] use for token-based auth to proxy

  # pass_through_endpoints:
  #   - path: "/v1/rerank"                                  # route you want to add to LiteLLM Proxy Server
  #     target: "https://api.cohere.com/v1/rerank"          # URL this route should forward requests to
  #     headers:                                            # headers to forward to this URL
  #       content-type: application/json                    # (Optional) Extra Headers to pass to this endpoint 
  #       accept: application/json
  #     forward_headers: True

# environment_variables:
  # settings for using redis caching
  # REDIS_HOST: redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com
  # REDIS_PORT: "16337"
  # REDIS_PASSWORD: 
